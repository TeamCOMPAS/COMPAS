{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The COMPAS core code gives a collection of CSV files.\n",
    "We convert it into a single h5 file which is more compact and easier to upload and handle\n",
    "\n",
    "\n",
    "To core-script is csv_to_h5.py placed in the H5 folder in the postprocessing\n",
    "and should be self-explanatory (:fingers_crossed:)\n",
    "\n",
    "This notebook is a remainder of the code construction and testing mostly for developers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for combining the COMPAS data into a single h5 file\n",
    "with some additional options. This is mainly is a remainder of the desting purposes\n",
    "\n",
    "\n",
    "Here we show how the code is constructed.\n",
    "Note that this is the combined effort of many group members hence no single\n",
    "person could make claim.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py  as h5  #for reading and writing h5 format\n",
    "import numpy as np  #for handling arrays\n",
    "import sys          #for handling paths and more\n",
    "import os           #for directory walking\n",
    "import subprocess as sp #for executing terminal command from python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The output from COMPAS\n",
    "\n",
    "These are the outputs in a single folder from a COMPAS run.\n",
    "The files are human-readable csv files and are separated into different topics.\n",
    "Here I list what we currently have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How the output is called from COMPAS\n",
    "\n",
    "fileNames = {1:'Compas_Log_BSE_Common_Envelopes.csv',\\\n",
    "           2:'Compas_Log_BSE_Double_Compact_Objects.csv',\\\n",
    "           3:'Compas_Log_BSE_RLOF.csv',\\\n",
    "           4:'Compas_Log_BSE_Supernovae.csv',\\\n",
    "           5:'Compas_Log_BSE_System_Parameters.csv',\\\n",
    "           6:'errorfile',\\\n",
    "           7:'output',\\\n",
    "          }\n",
    "\n",
    "#How the groups will be called in H5\n",
    "\n",
    "groupNames = {1:'CommonEnvelopes',\\\n",
    "              2:'DoubleCompactObjects',\\\n",
    "              3:'RLOF',\\\n",
    "              4:'Supernovae',\\\n",
    "              5:'SystemParameters',\\\n",
    "              6:'errorFile',\\\n",
    "              7:'output'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining folders\n",
    "\n",
    "In the case of a large simulation we run it on multiple cores.\n",
    "Each core runs in its own subfolder of a parent folder of the simulation.\n",
    "Before we write an h5 file, we want to combine all the files.\n",
    "\n",
    "Main idea is to use a directory walker and write to a single csv.\n",
    "Main trickyness is to write the header only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineOutputsOfFile(baseDirectoryData='.', groups=[1,2,3,4]):\n",
    "    \"\"\"\n",
    "    For a simulation in folder baseDirectory\n",
    "    1 - write a new file name=Combined_'filename'\n",
    "    2 - go through all the subfolders and find the filename\n",
    "    3 - Of the first file with filename, copy the header \n",
    "        and write the data to Combined_'filename'\n",
    "    4 - Of other files with filename, copy the data\n",
    "    5 - close the newly written file\n",
    "    \n",
    "    input = filename , string(with extension)\n",
    "    \n",
    "    COMPAS has a strict header format which is assumed here\n",
    "    \n",
    "    1st line = type data (INT, FLOAT BOOL etc)\n",
    "    2nd line = unit data (Msol, ergs,Tsol etc)\n",
    "    3th line = column name\n",
    "    \"\"\"\n",
    "    for group in groups:\n",
    "        filename       = fileNames[group]\n",
    "                         \n",
    "        #1-------\n",
    "        combinedOutput = open(baseDirectoryData+'/Combine_'+filename, 'w')\n",
    "        #boolean to see if we have written the header\n",
    "        headersWritten = False\n",
    "        nHeaders       = 3\n",
    "        nColumnCheck   = None #check each line if nr of entries is the same\n",
    "                              #if not there is somthing wrong\n",
    "        #2---- \n",
    "        for root,dirs,files in os.walk(baseDirectoryData):\n",
    "\n",
    "            for f in files:\n",
    "\n",
    "                if f == filename:\n",
    "\n",
    "                    path = os.path.join(root, f)\n",
    "\n",
    "                    #individual output file of run in subfolder\n",
    "                    outputFile   = open(path)\n",
    "                    #3--------------\n",
    "                    if not headersWritten:\n",
    "                        for i in range(nHeaders):\n",
    "                            line = outputFile.readline()\n",
    "\n",
    "\n",
    "                            line  = line.replace(\" \", \"\")\n",
    "                            line  = line.replace(\",\", \"\\t\")\n",
    "                            nCols = len(line.split('\\t'))\n",
    "                            if i ==0: #set the column number check for first line\n",
    "                                nColumnCheck = nCols\n",
    "                            if(nCols != nColumnCheck):\n",
    "                                raise ValueError('wrong number of columns in header=%s'%(i))\n",
    "                            combinedOutput.write(line)\n",
    "                        headersWritten = True\n",
    "                    else:\n",
    "                        #skip the header by reading and not doing anything\n",
    "                        [outputFile.readline() for i in range(nHeaders)]\n",
    "\n",
    "                    #4 -----------\n",
    "                    for line in outputFile:\n",
    "                        nCols = len(line.split(','))\n",
    "                        line  = line.replace(\",\", \"\\t\")\n",
    "                        if(nCols != nColumnCheck):\n",
    "                            raise ValueError('wrong number of columns in data')\n",
    "                        combinedOutput.write(line)\n",
    "        #5----------\n",
    "        combinedOutput.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the H5 file\n",
    "\n",
    "This writes the combined files to an H5file, note that even for a\n",
    "single simulation on a single core this works, since CombinedFile==Original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addHdf5HeadersAndAttributes(hf,  groupName, filePath):\n",
    "    \"\"\"\n",
    "    COMPAS has a strict header format which is assumed here\n",
    "    \n",
    "    1st line = type data (INT, FLOAT BOOL etc)\n",
    "    2nd line = unit data (Msol, ergs,Tsol etc)\n",
    "    3th line = column name\n",
    "    \"\"\"\n",
    "    \n",
    "    file      = open(filePath, 'r')\n",
    "    #get header, units names\n",
    "    types     = file.readline()[:-1].split('\\t')\n",
    "    units     = file.readline()[:-1].split('\\t')\n",
    "    headers   = file.readline()[:-1].split('\\t')\n",
    "    #how many entries will a column in the group have?\n",
    "    #get the length of the file (minus headers)\n",
    "    fileLength = int(sp.check_output('wc -l ' + filePath, \\\n",
    "                                     shell=True).split()[0]) - 3\n",
    "    file.close() # only needed the headers here\n",
    "    #types is strings need to replace by actual type for h5\n",
    "    dtypes    = []\n",
    "    print(headers)\n",
    "    for nrt, typ in enumerate(types):\n",
    "        if typ == 'INT':\n",
    "            dtypes.append(np.int64)\n",
    "        elif typ == 'FLOAT':\n",
    "            dtypes.append(np.float64)\n",
    "        elif typ == 'BOOL':\n",
    "            dtypes.append(bool)\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognised datatype typ=%s - for column %s in file%s \"\\\n",
    "                             %(typ, headers[nrt], groupName))\n",
    "    #create the groups in the h5file and add units and explanation string\n",
    "    for header,dtype,unit in zip(headers,dtypes,units):\n",
    "        dset = hf[groupName].create_dataset(header,dtype=dtype,shape=(fileLength,))\n",
    "        dset.attrs['units'] = unit\n",
    "        #dset.attrs['comment']= columnDescriptions\n",
    "        \n",
    "    return\n",
    "\n",
    "def addHdf5Data(hf,  groupName, filePath):\n",
    "    \n",
    "    #too slow to go line by line, so load in a modest \n",
    "    #(in term sof memory) amount at a time\n",
    "    chunkSize = 500000\n",
    "    \n",
    "    #get the length of the file (minus headers)\n",
    "    fileLength = int(sp.check_output('wc -l ' + filePath, \\\n",
    "                                      shell=True).split()[0]) - 3\n",
    "\n",
    "    file      = open(filePath)\n",
    "    types     = file.readline()[:-1].split('\\t')\n",
    "    units     = file.readline()[:-1].split('\\t')\n",
    "    headers   = file.readline()[:-1].split('\\t')\n",
    "    \n",
    "    nrColumns   = len(headers)\n",
    "    group       = hf[groupName]\n",
    "    chunkBegin = 0\n",
    "    chunkEnd = 0    \n",
    "    while chunkEnd < fileLength:\n",
    "        data = []\n",
    "\n",
    "        chunkEnd = chunkBegin + chunkSize\n",
    "\n",
    "        #dont try to load in more data than you've got\n",
    "        if chunkEnd > fileLength:\n",
    "            chunkEnd = fileLength\n",
    "\n",
    "        #read in a modest number of lines\n",
    "        for i in range(chunkEnd-chunkBegin):\n",
    "            data.append(file.readline()[:-1].split())\n",
    "            \n",
    "        data = np.array(data)\n",
    "            \n",
    "        #data is now a 2d array where each column is a specific variable\n",
    "\n",
    "        \n",
    "        for nrcolumn in range(nrColumns):\n",
    "            #fill in the values in the preshaped array\n",
    "            #(see dset addHdf5HeadersAndAttributes() )\n",
    "            \n",
    "            columnName        = headers[nrcolumn]\n",
    "            dtype             = type(group[columnName][0])\n",
    "            group[columnName][chunkBegin:chunkEnd] = np.array(data[:,nrcolumn],dtype=dtype)\n",
    "            \n",
    "        chunkBegin = chunkEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def createH5file(baseDirectoryData='.', groups=[1,2,3,4], h5Name='COMPAS_output.h5'):\n",
    "    \n",
    "    hf = h5.File(baseDirectoryData+'/'+h5Name, 'w')\n",
    "    \n",
    "    #use the groupNames dictionary to create \n",
    "    #the H5file name group. Each of these we will fill with the header and data\n",
    "    for groupNumber in groups:\n",
    "        groupName  = groupNames[groupNumber]\n",
    "        fileName   = '/Combine_'+fileNames[groupNumber]\n",
    "        filePath   = baseDirectoryData+fileName\n",
    "        hf.create_group(groupName)\n",
    "        addHdf5HeadersAndAttributes(hf, groupName, filePath)\n",
    "        addHdf5Data(hf, groupName, filePath)\n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUpInIsleNumber2Please(baseDirectoryData='.', groups=[1,2,3,4]):\n",
    "    \n",
    "    for groupNumber in groups:\n",
    "        fileName   = '/Combine_'+fileNames[groupNumber]\n",
    "        filePath   = baseDirectoryData+fileName\n",
    "        command    = 'rm '+filePath\n",
    "        sp.Popen(command, shell=True, executable='/bin/bash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'SEED', 'Time', 'CE_Alpha', 'Lambda@CE_1', 'Lambda@CE_2', 'Binding_Energy<CE_1', 'Binding_Energy<CE_2', 'Eccentricity<CE', 'Eccentricity>CE', 'Separation<CE', 'Separation>CE', 'RocheLobe_1<CE', 'RocheLobe_1>CE', 'RocheLobe_2<CE', 'RocheLobe_2>CE', 'MZAMS_1', 'Mass_1<CE', 'Mass_Env_1', 'Core_Mass_1', 'Radius_1<CE', 'Radius_1>CE', 'Stellar_Type_1<CE', 'Stellar_Type_1', 'Lambda_Fixed_1', 'Lambda_Nanjing_1', 'Loveridge_1', 'Loveridge_Winds_1', 'Kruckow_1', 'BE_Fixed_1', 'BE_Nanjing_1', 'BE_Loveridge_1', 'BE_Loveridge_Winds_1', 'BE_Kruckow_1', 'MZAMS_2', 'Mass_2<CE', 'Mass_Env_2', 'Core_Mass_2', 'Radius_2<CE', 'Radius_2>CE', 'Stellar_Type_2<CE', 'Stellar_Type_2', 'Lambda_Fixed_2', 'Lambda_Nanjing_2', 'Loveridge_2', 'Loveridge_Winds_2', 'Kruckow_2', 'BE_Fixed_2', 'BE_Nanjing_2', 'BE_Loveridge_2', 'BE_Loveridge_Winds_2', 'BE_Kruckow_2', 'MT_History', 'Merger', 'Optimistic_CE', 'CE_Event_Count', 'Double_Core_CE', 'RLOF_1', 'Luminosity<CE_1', 'Teff<CE_1', 'Tau_Dynamical<CE_1', 'Tau_Thermal<CE_1', 'Tau_Nuclear<CE_1', 'RLOF_2', 'Luminosity<CE_2', 'Teff<CE_2', 'Tau_Dynamical<CE_2', 'Tau_Thermal<CE_2', 'Tau_Nuclear<CE_2', 'Zeta_Star_Compare', 'Zeta_RLOF_Analytic', 'Tau_Sync', 'Tau_Circ', 'Tau_Radial<CE_1', 'Tau_Radial<CE_2', 'Immediate_RLOF>CE', 'Simultaneous_RLOF']\n",
      "['ID', 'SEED', 'Separation', 'Eccentricity_0', 'Separation<2ndSN', 'Eccentricity<2ndSN', 'Orb_Velocity<2ndSN', 'Separation@DCO', 'Eccentricity@DCO', 'Metallicity_1', 'Metallicity_2', 'MZAMS_1', 'Mass_0_1', 'Mass_Total@CO_1', 'Mass_He_Core@CO_1', 'Mass_CO_Core@CO_1', 'Mass_Core@CO_1', 'Mass_He_Core@CE_1', 'Mass_CO_Core@CE_1', 'Mass_Core@CE_1', 'Kick_Velocity_1', 'SN_Theta_1', 'SN_Phi_1', 'Mass_1', 'Stellar_Type_ZAMS_1', 'Stellar_Type_1', 'MZAMS_2', 'Mass_0_2', 'Mass_Total@CO_2', 'Mass_He_Core@CO_2', 'Mass_CO_Core@CO_2', 'Mass_Core@CO_2', 'Mass_He_Core@CE_2', 'Mass_CO_Core@CE_2', 'Mass_Core@CE_2', 'Kick_Velocity_2', 'SN_Theta_2', 'SN_Phi_2', 'Mass_2', 'Stellar_Type_ZAMS_2', 'Stellar_Type_2', 'Coalescence_Time', 'Time', 'LBV_Multiplier', 'Sigma_Kick_CCSN_NS', 'Sigma_Kick_CCSN_BH', 'CE_Alpha', 'WR_Multiplier', 'RLOF_Secondary>CE', 'MT_Case_1', 'MT_Case_2', 'Orbital_Energy<SN_1', 'Orbital_Energy>SN_1', 'Orbital_Energy<SN_2', 'Orbital_Energy>SN_2', 'CEE', 'Lambda@CE_1', 'Lambda@CE_2', 'Eccentricity<CE', 'Eccentricity>CE', 'Separation<CE', 'Separation>CE', 'RocheLobe_1<CE', 'RocheLobe_1>CE', 'RocheLobe_2<CE', 'RocheLobe_2>CE', 'Optimistic_CE', 'Merges_Hubble_Time', 'Double_Core_CE', 'Binding_Energy@CE_1', 'Binding_Energy@CE_2', 'Recycled_NS_1', 'Recycled_NS_2', 'ECSN_1', 'ECSN_2', 'Experienced_ECSN_1', 'Experienced_ECSN_2', 'Experienced_PISN_1', 'Experienced_PISN_2', 'Experienced_PPISN_1', 'Experienced_PPISN_2']\n",
      "['ID', 'SEED', 'Mass_1', 'Mass_2', 'Radius_1', 'Radius_2', 'Type_1', 'Type_2', 'Separation', 'Event_Counter', 'Time', 'RLOF_1', 'RLOF_2', 'CEE', 'Mass_1_Prev', 'Mass_2_Prev', 'Radius_1_Prev', 'Radius_2_Prev', 'Type_1_Prev', 'Type_2_Prev', 'Separation_Prev', 'EventCounter_Prev', 'Time_Prev', 'RLOF_1_Prev', 'RLOF_2_Prev', 'Zeta_Thermal_1', 'Zeta_Nuclear_1', 'Zeta_Soberman_1', 'Zeta_SoberMan_He_1', 'Zeta_Hurley_1', 'Zeta_Hurley_He_1', 'Zeta_Thermal_2', 'Zeta_Nuclear_2', 'Zeta_Soberman_2', 'Zeta_SoberMan_He_2', 'Zeta_Hurley_2', 'Zeta_Hurley_He_2', 'Zeta_RLOF_Analytic', 'Zeta_RLOF_Numerical']\n",
      "['ID', 'SEED', 'Drawn_Kick_Velocity_SN', 'Kick_Velocity_SN', 'Fallback_Fraction_SN', 'Orbital_Velocity', 'Kick_Velocity(uK)', 'True_Anomaly(psi)_SN', 'SN_Theta_SN', 'SN_Phi_SN', 'USSN_SN', 'SN_SN', 'ECSN_SN', 'Experienced_PISN_SN', 'Experienced_PPISN_SN', 'Survived_SN_Event', 'MZAMS_SN', 'MZAMS_CP', 'Mass_Total@CO_SN', 'Mass_CP', 'Mass_CO_Core@CO_SN', 'Mass_SN', 'Experienced_RLOF_SN', 'Stellar_Type_ZAMS_SN', 'Stellar_Type_SN', 'Supernova_State', 'Stellar_Type_Prev_SN', 'Stellar_Type_Prev_CP', 'Mass_Core@CO_SN', 'Metallicity_SN', 'CEE', 'Stable_RLOF>CE', 'RLOF->NS_SN', 'Time', 'Eccentricity<2ndSN', 'Eccentricity', 'Separation<2ndSN', 'Separation', 'Systemic_Velocity', 'Hydrogen_Rich_SN', 'Hydrogen_Poor_SN', 'Runaway_CP']\n",
      "['ID', 'SEED', 'MZAMS_1', 'MZAMS_2', 'Mass_0_1', 'Mass_0_2', 'Separation', 'Eccentricity_0', 'SN_Kick_VM_Rand_1', 'SN_Theta_1', 'SN_Phi_1', 'Mean_Anomaly_1', 'SN_Kick_VM_Rand_2', 'SN_Theta_2', 'SN_Phi_2', 'Mean_Anomaly_2', 'Omega_ZAMS_1', 'Omega_ZAMS_2', 'Sigma_Kick_CCSN_NS', 'Sigma_Kick_CCSN_BH', 'Sigma_Kick_ECSN', 'Sigma_Kick_USSN', 'LBV_Multiplier', 'WR_Multiplier', 'CE_Alpha', 'Metallicity_1', 'Metallicity_2', 'Secondary<<DCO', 'Unbounded', 'Merger', 'Merger_At_Birth', 'Stellar_Type_ZAMS_1', 'Stellar_Type_1', 'Stellar_Type_ZAMS_2', 'Stellar_Type_2', 'Error']\n"
     ]
    }
   ],
   "source": [
    "# location where .csv files live\n",
    "pathToData =  '/Users/floorbroekgaarden/Programs/githubCOMPAS/COMPAS/src'\n",
    "groups = [1,2,3,4,5]\n",
    "combineOutputsOfFile(baseDirectoryData=pathToData, groups=groups)\n",
    "createH5file(baseDirectoryData=pathToData, groups=groups)\n",
    "cleanUpInIsleNumber2Please(baseDirectoryData=pathToData, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[-0.4064324   0.02165689  0.4440852  ...  0.4121895   0.6409085\n",
      "  0.09198198]\n"
     ]
    }
   ],
   "source": [
    "# as a test to see if the output file has been created, read in a line of the supernova data\n",
    "f  = h5.File(pathToData+'/COMPAS_output.h5')\n",
    "print()\n",
    "print(f['Supernovae']['SN_Theta_SN'][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
